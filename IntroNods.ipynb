{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IntroNods.ipynb","provenance":[],"mount_file_id":"1nUCNa2sDOJr5ue8t66kE8CI-ppUhqntn","authorship_tag":"ABX9TyMV9F2041Z8c/xlsgK4KGiH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import lib"],"metadata":{"id":"lnP7KJ1uD16G"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"k_ZY4oioDx8T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Code donné pour la fonction d'entrainement du l'algo\n","\n"],"metadata":{"id":"3361Cwo3DwCB"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3J5tQTJvDahN","executionInfo":{"status":"ok","timestamp":1654071563922,"user_tz":-120,"elapsed":605,"user":{"displayName":"william pierre","userId":"04724092207929898858"}},"outputId":"64a85a03-f9ad-416c-c297-57e800d0f620"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.2  0.6  0.3] 4\n"]}],"source":["def h(x):\n","  if x > 0:\n","    return 1\n","  else:\n","    return 0\n","\n","def neuron_train(X, Y, w, µ):\n","  it      = 0\n","  is_over = False\n","  while is_over is False:\n","    is_over = True\n","\n","    for x, y_ex in zip(X, Y):\n","      a = np.inner(w, x) # vector product\n","      y = h(a)\n","      e = y_ex - y\n","\n","      w = w + µ * e * x\n","\n","      if e != 0:\n","        is_over = False\n","\n","    it += 1\n","\n","  return w, it\n","\n","w = np.array([0.3, 0.1, -0.2])\n","X = np.array([ # we add 1 to represent the bias\n","  [1, 0, 0],\n","  [1, 1, 0],\n","  [1, 0, 1],\n","  [1, 1, 1]\n","])\n","y = np.array([0, 1, 1, 1])\n","µ = 0.5\n","\n","w, it = neuron_train(X, y, w, µ)\n","print(w, it)"]},{"cell_type":"markdown","source":["Fonction qui sert à générer des données"],"metadata":{"id":"DwzhC6fIXSMt"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","import math\n","\n","def generate_dataset(n_samples):\n","  # nous générons des vecteurs de taille 2 dont les valeurs sont entre -1 et 1\n","  x = torch.rand(n_samples, 2) * 2 - 1\n","\n","  # nous calculons la distance entre le point et l'origine\n","  x_norms = (x ** 2).sum(axis = 1).sqrt()\n","\n","  # le rayon du disque\n","  radius = math.sqrt(2 / math.pi)\n","\n","  # compare la norme au rayon du disque, créé un masque booléen,\n","  # puis trasnforme en entier 8 octects.\n","  y = (x_norms < radius).long()\n","\n","  return TensorDataset(x, y)"],"metadata":{"id":"zihP2Yj7XTFA","executionInfo":{"status":"ok","timestamp":1654071842742,"user_tz":-120,"elapsed":3723,"user":{"displayName":"william pierre","userId":"04724092207929898858"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Création d'un modèle perceptron multicouche. Il y a deux couches de 64 neurones"],"metadata":{"id":"H0CNQ68YXjqW"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","  def __init__(self):\n","    super(MLP, self).__init__()\n","    self.hidden_layer_1    = nn.Linear(2, 64)       #2 connexions en entrée 64 en sortie\n","    self.hidden_layer_2    = nn.Linear(64, 64)      #64 connexions en entrée 64 en sortie\n","    self.output_layer      = nn.Linear(64, 2)       #64 connexions en entrée 2 en sortie\n","    self.hidden_transfer_1 = nn.ReLU()\n","    self.hidden_transfer_2 = nn.ReLU()\n","    self.output_transfer   = nn.LogSoftmax(dim = 1)\n","\n","  def forward(self, x):\n","    x = self.hidden_layer_1(x)\n","    x = self.hidden_transfer_1(x)\n","    x = self.hidden_layer_2(x)\n","    x = self.hidden_transfer_2(x)\n","    x = self.output_layer(x)\n","    x = self.output_transfer(x)\n","\n","    return x"],"metadata":{"id":"OtPePoklXklZ","executionInfo":{"status":"ok","timestamp":1654074599221,"user_tz":-120,"elapsed":224,"user":{"displayName":"william pierre","userId":"04724092207929898858"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Test de précion du résultat de chaque neurne"],"metadata":{"id":"Yd4UOOGaeEM2"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train(\n","  model, loss_func, optimizer, train_dataset,\n","  test_dataset, epochs, batch_size\n","):\n","  dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n","\n","  for epoch in range(epochs):\n","    for X, y in dataloader:\n","      y_pred = model(X)\n","      loss   = loss_func(y_pred, y)\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","    if epoch % 10 == 0:\n","      train_accuracy = evaluate(model, train_dataset, batch_size)\n","      test_accuracy  = evaluate(model, test_dataset, batch_size)\n","      print(\n","        f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n","        f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n","      )"],"metadata":{"id":"5-ByJAtHeEvg","executionInfo":{"status":"ok","timestamp":1654074601723,"user_tz":-120,"elapsed":313,"user":{"displayName":"william pierre","userId":"04724092207929898858"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"L5L9PTofgBZs"}},{"cell_type":"code","source":["def evaluate(model, dataset, batch_size):\n","  dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)\n","\n","  correct_pred = 0\n","  total_pred   = 0\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      y_pred        = model(X)\n","      y_pred_class  = y_pred.argmax(dim = 1)\n","      correct_pred += (y_pred_class == y).sum().item()\n","      total_pred   += len(y)\n","\n","  return correct_pred / total_pred"],"metadata":{"id":"IJSuZz1HgB8w","executionInfo":{"status":"ok","timestamp":1654075921771,"user_tz":-120,"elapsed":212,"user":{"displayName":"william pierre","userId":"04724092207929898858"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["Initialisation du modèle d'entrainement. Exécution du modèle\n","\n"],"metadata":{"id":"4SGalr3yXpcQ"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","n_samples     = 3000\n","epochs        = 300\n","batch_size    = 32\n","learning_rate = 1e-3\n","\n","train_dataset = generate_dataset(n_samples)\n","test_dataset  = generate_dataset(n_samples)\n","model         = MLP()\n","loss_func     = nn.NLLLoss()\n","optimizer     = optim.SGD(\n","  params = model.parameters(),\n","  lr     = learning_rate\n",")\n","train(\n","  model, loss_func, optimizer, train_dataset,\n","  test_dataset, epochs, batch_size\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx-54Ni_Xp3s","executionInfo":{"status":"ok","timestamp":1654075952621,"user_tz":-120,"elapsed":29021,"user":{"displayName":"william pierre","userId":"04724092207929898858"}},"outputId":"0839c266-accd-4a3f-fac8-405811b29dc1"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["  0 -> 49.367% train accuracy   0 -> 50.467% test accuracy\n"," 10 -> 61.500% train accuracy  10 -> 61.567% test accuracy\n"," 20 -> 67.833% train accuracy  20 -> 67.167% test accuracy\n"," 30 -> 71.633% train accuracy  30 -> 70.767% test accuracy\n"," 40 -> 76.967% train accuracy  40 -> 76.267% test accuracy\n"," 50 -> 81.800% train accuracy  50 -> 80.200% test accuracy\n"," 60 -> 84.167% train accuracy  60 -> 83.167% test accuracy\n"," 70 -> 86.733% train accuracy  70 -> 86.500% test accuracy\n"," 80 -> 89.167% train accuracy  80 -> 89.200% test accuracy\n"," 90 -> 90.800% train accuracy  90 -> 91.033% test accuracy\n","100 -> 92.533% train accuracy 100 -> 92.433% test accuracy\n","110 -> 93.733% train accuracy 110 -> 93.100% test accuracy\n","120 -> 94.333% train accuracy 120 -> 93.933% test accuracy\n","130 -> 94.700% train accuracy 130 -> 94.433% test accuracy\n","140 -> 95.133% train accuracy 140 -> 94.900% test accuracy\n","150 -> 95.367% train accuracy 150 -> 95.167% test accuracy\n","160 -> 95.800% train accuracy 160 -> 95.900% test accuracy\n","170 -> 96.100% train accuracy 170 -> 96.033% test accuracy\n","180 -> 96.367% train accuracy 180 -> 96.133% test accuracy\n","190 -> 96.600% train accuracy 190 -> 96.433% test accuracy\n","200 -> 96.733% train accuracy 200 -> 96.733% test accuracy\n","210 -> 96.833% train accuracy 210 -> 96.900% test accuracy\n","220 -> 97.300% train accuracy 220 -> 97.033% test accuracy\n","230 -> 97.467% train accuracy 230 -> 97.033% test accuracy\n","240 -> 97.667% train accuracy 240 -> 97.200% test accuracy\n","250 -> 97.667% train accuracy 250 -> 97.067% test accuracy\n","260 -> 97.733% train accuracy 260 -> 97.167% test accuracy\n","270 -> 98.167% train accuracy 270 -> 97.233% test accuracy\n","280 -> 98.133% train accuracy 280 -> 97.200% test accuracy\n","290 -> 98.333% train accuracy 290 -> 97.333% test accuracy\n"]}]}]}